{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "squareSpace",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jibram/artificialIntelligence/blob/master/languageDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "l14NO9yrcemj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import copy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "import keras.optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "# Character occurance\n",
        "def character_occ(data, characters):\n",
        "  for i in range(0, len(data)):\n",
        "    charList = [0]*len(characters)\n",
        "    for char in data[i]:\n",
        "      charList[characters.index(char)] += 1\n",
        "    data[i] = charList\n",
        "  return data\n",
        "\n",
        "# Makes a unique set of all the languages in the data.\n",
        "def get_languages(data):\n",
        "  languages = dict()\n",
        "  for line in data:\n",
        "    if line['classification'] not in languages:\n",
        "      languages[line['classification']] = len(languages)\n",
        "  return languages\n",
        "\n",
        "# Makes a list of all possible characters from data.\n",
        "def get_characters(data):\n",
        "  characters = []\n",
        "  for line in data:\n",
        "    for char in line['text']:\n",
        "      if char not in characters:\n",
        "        characters.append(char)\n",
        "  return characters\n",
        "\n",
        "# Makes a list of json objects into a list of strings.\n",
        "def listify(data, key):\n",
        "  newList = []\n",
        "  for line in data:\n",
        "    newList.append(line[key])\n",
        "  return newList\n",
        "\n",
        "# Convert list of lists to numpy array of dimension len(data) x 140\n",
        "def trimData(data):\n",
        "  for i in range(0,len(data)):\n",
        "    if len(data[i]) < 140:\n",
        "      data[i] = data[i].ljust(140,'?')\n",
        "    else:\n",
        "      data[i] = data[i][:140]\n",
        "  return data\n",
        "\n",
        "def get_data():\n",
        "  # Prepare the data into lists, parsing the jsons. Since we are using python3, it allows unicode.\n",
        "  data_str = []\n",
        "  with open(\"train_X_languages_homework.json.txt\") as f:\n",
        "    for line in f:\n",
        "      data_str.append(json.loads(line))\n",
        "\n",
        "  data_ans = []\n",
        "  with open(\"train_y_languages_homework.json.txt\") as f:\n",
        "    for line in f:\n",
        "      data_ans.append(json.loads(line))\n",
        "\n",
        "  # Set of all languages\n",
        "  characters = get_characters(data_str)\n",
        "  languages = get_languages(data_ans)\n",
        "  \n",
        "  # Remove all extra json formatting and just get list of all strings\n",
        "  data_str = listify(data_str, 'text')\n",
        "  data_ans = listify(data_ans, 'classification')\n",
        "  \n",
        "  # Standardize our data\n",
        "  data_str = trimData(data_str)\n",
        "  data_ans = [languages[langCode] for langCode in data_ans]\n",
        "\n",
        "  return data_str, data_ans, characters, languages # Bag of characters size is 5697. Add 1 for bias, 5698 inputs per line.\n",
        "\n",
        "  \n",
        "def build_model():\n",
        "  # data_str contains list of all strings\n",
        "  # data_labels contains list of all language codes\n",
        "  # characters is a map of all characters used in data_str\n",
        "  # languages is a list with language codes. Useful to correspond with an index value\n",
        "  data_str, data_labels, characters, languages = get_data()\n",
        "  \n",
        "  partialData = character_occ(data_str[:], characters)\n",
        "  partialLabels = data_labels[:]\n",
        "  input_size = len(partialData[0])\n",
        "  del data_str, data_labels\n",
        "  \n",
        "  BATCH_SIZE = 512\n",
        "  EPOCHS = 12\n",
        "  \n",
        "  NUM_SAMPLES = len(partialData)\n",
        "  VOCAB_SIZE = len(characters)\n",
        "  \n",
        "  X = np.array(partialData, dtype=np.float32)\n",
        "  Y = np.array(partialLabels, dtype=np.float32)\n",
        "  del partialData, partialLabels\n",
        "           \n",
        "  \n",
        "  standard_scaler = preprocessing.StandardScaler().fit(X)\n",
        "  X = standard_scaler.transform(X)\n",
        "  \n",
        "  Y = keras.utils.to_categorical(Y, num_classes=len(languages))\n",
        " \n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
        "  del X, Y\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(500, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(300, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(100, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(len(languages), kernel_initializer=\"glorot_uniform\", activation=\"softmax\"))\n",
        "  model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "  model.compile(loss='categorical_crossentropy', \n",
        "                optimizer=model_optimizer,\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  tensorboard = TensorBoard(log_dir=\"run\")\n",
        "  \n",
        "  history = model.fit(X_train,Y_train,\n",
        "                     epochs=EPOCHS,\n",
        "                     validation_split=0.1,\n",
        "                     batch_size=BATCH_SIZE,\n",
        "                     callbacks=[tensorboard],\n",
        "                     verbose=2)\n",
        "  \n",
        "  scores = model.evaluate(X_test, Y_test, verbose=1)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  \n",
        "  # 1 This is the build model script\n",
        "  # 3 Serialize the Model\n",
        "  \n",
        "  # serialize model to JSON\n",
        "  model_json = model.to_json()\n",
        "  with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(\"model.h5\")\n",
        "  \n",
        "  # 5 Save the performance information\n",
        "  with open(\"performance.txt\", \"w\") as f:\n",
        "    f.write(\"Model is expected to be correct roughly about \" + str(scores[1]*100) + \"%. \")\n",
        "  with open(\"performance.txt\", \"a+\") as f:\n",
        "    f.write(\"Uses Dropout after each dense layer and an adam optimizer. 80/20\")\n",
        "    f.write(\"train/test ratio with sigmoid activations. Loss function is\")\n",
        "    f.write(\"categorical crossentropy. 12 Epochs of Batch Size 512.\")\n",
        "  \n",
        "build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQgEenHEc9Jr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import copy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import keras\n",
        "import keras.optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "# Character occurance\n",
        "def character_occ(data, characters):\n",
        "  for i in range(0, len(data)):\n",
        "    charList = [0]*len(characters)\n",
        "    for char in data[i]:\n",
        "      if char in characters:\n",
        "        charList[characters.index(char)] += 1\n",
        "      else:\n",
        "        charList[characters.index('?')] += 1\n",
        "    data[i] = charList\n",
        "  return data\n",
        "\n",
        "# Makes a list of all possible characters from data.\n",
        "def get_characters(data):\n",
        "  characters = []\n",
        "  for line in data:\n",
        "    for char in line['text']:\n",
        "      if char not in characters:\n",
        "        characters.append(char)\n",
        "  return characters\n",
        "\n",
        "# Makes a unique set of all the languages in the data.\n",
        "def get_languages(data):\n",
        "  languages = dict()\n",
        "  for line in data:\n",
        "    if line['classification'] not in languages:\n",
        "      languages[line['classification']] = len(languages)\n",
        "  return languages\n",
        "\n",
        "# Convert list of lists to numpy array of dimension len(data) x 140\n",
        "def trimData(data):\n",
        "  for i in range(0,len(data)):\n",
        "    if len(data[i]) < 140:\n",
        "      data[i] = data[i].ljust(140,'?')\n",
        "    else:\n",
        "      data[i] = data[i][:140]\n",
        "  return data\n",
        "\n",
        "# Makes a list of json objects into a list of strings.\n",
        "def listify(data, key):\n",
        "  newList = []\n",
        "  for line in data:\n",
        "    newList.append(line[key])\n",
        "  return newList\n",
        "\n",
        "def get_data():\n",
        "  # Prepare the data into lists, parsing the jsons. Since we are using python3, it allows unicode.\n",
        "  data_str = []\n",
        "  with open(\"test_X_languages_homework.json.txt\") as f:\n",
        "    for line in f:\n",
        "      data_str.append(json.loads(line))\n",
        "      \n",
        "  train_str = []\n",
        "  with open(\"train_X_languages_homework.json.txt\") as f:\n",
        "    for line in f:\n",
        "      train_str.append(json.loads(line))\n",
        "      \n",
        "  train_ans = []\n",
        "  with open(\"train_y_languages_homework.json.txt\") as f:\n",
        "    for line in f:\n",
        "      train_ans.append(json.loads(line))\n",
        "      \n",
        "  # Set of all languages\n",
        "  characters = get_characters(train_str)\n",
        "  languages = get_languages(train_ans)\n",
        "  languages = dict([[v,k] for k,v in languages.items()])\n",
        "  \n",
        "  # Remove all extra json formatting and just get list of all strings\n",
        "  data_str = listify(data_str, 'text')\n",
        "  train_str = listify(train_str, 'text')\n",
        "  \n",
        "  # Standardize our data\n",
        "  data_str = trimData(data_str)\n",
        "  train_str = trimData(train_str)\n",
        "\n",
        "  return data_str, train_str, characters, languages\n",
        "\n",
        "  \n",
        "def make_predictions():\n",
        "  \n",
        "  # Prepare the test data.\n",
        "  test_data, train_str, characters, languages = get_data()\n",
        "  \n",
        "  test_data = character_occ(test_data, characters)\n",
        "\n",
        "  # load json and create model\n",
        "  with open('model.json', 'r') as json_file:\n",
        "    loaded_model_json = json_file.read()\n",
        "  model = model_from_json(loaded_model_json)\n",
        "                          \n",
        "  # load weights into new model\n",
        "  model.load_weights(\"model.h5\")\n",
        "  \n",
        "  # evaluate loaded model on test data\n",
        "  model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "  model.compile(loss='categorical_crossentropy', \n",
        "                      optimizer=model_optimizer,\n",
        "                      metrics=['accuracy'])\n",
        "  \n",
        "  X = np.array(test_data, dtype=np.float32)\n",
        "  del test_data, train_str, characters\n",
        "  standard_scaler = preprocessing.StandardScaler().fit(X)\n",
        "  X = standard_scaler.transform(X)\n",
        "  \n",
        "  classes = model.predict_classes(X, batch_size=10)\n",
        "  classes = classes.tolist()\n",
        "  with open('predictions.txt', 'w') as f:\n",
        "    for item in classes:\n",
        "        f.write(\"%s\\n\" % languages[item])\n",
        "  \n",
        "make_predictions()\n",
        "                          \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}